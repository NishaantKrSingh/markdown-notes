<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Markdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="refresh" content="3">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@600&display=swap" rel="stylesheet">
    <style>
        body{
            background-color: rgb(56, 56, 56);
            color: whitesmoke;
             font-family: "Urbanist", sans-serif;
            font-optical-sizing: auto;
            font-weight: 600;
            font-style: normal;
        };

        /* pre{
             font-family: "Urbanist", sans-serif;
             font-optical-sizing: auto;
             font-weight: 600;
             font-style: normal;
        } */
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/night-owl.min.css">
    <title>Merkdown</title>
</head>
<body><h1 id="voicecraftzeroshotspeecheditingandtexttospeechinthewild">VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</h1>
<p><a href="https://jasonppy.github.io/VoiceCraft_web">Demo</a> <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">Paper</a></p>
<h3 id="tldr">TL;DR</h3>
<p>VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both <strong>speech editing</strong> and <strong>zero-shot text-to-speech (TTS)</strong> on in-the-wild data including audiobooks, internet videos, and podcasts.</p>
<p>To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.</p>
<h2 id="news">News</h2>
<p>:star: 03/28/2024: Model weights are up on HuggingFaceü§ó <a href="https://huggingface.co/pyp1/VoiceCraft/tree/main">here</a>!</p>
<h2 id="todo">TODO</h2>
<ul>
<li>[x] Codebase upload</li>
<li>[x] Environment setup</li>
<li>[x] Inference demo for speech editing and TTS</li>
<li>[x] Training guidance</li>
<li>[x] RealEdit dataset and training manifest</li>
<li>[x] Model weights (both 330M and 830M, the former seems to be just as good)</li>
<li>[ ] Write colab notebooks for better hands-on experience</li>
<li>[ ] HuggingFace Spaces demo</li>
<li>[ ] Better guidance on training</li>
</ul>
<h2 id="howtorunttsinference">How to run TTS inference</h2>
<p>There are two ways: </p>
<ol>
<li>with docker. see <a href="#quickstart">quickstart</a></li>
<li>without docker. see <a href="#environment-setup">envrionment setup</a></li>
</ol>
<p>When you are inside the docker image or you have installed all dependencies, Checkout <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a>.</p>
<h2 id="quickstart">QuickStart</h2>
<p>:star: To try out TTS inference with VoiceCraft, the best way is using docker. Thank <a href="https://github.com/ubergarm">@ubergarm</a> and <a href="https://github.com/jay-c88">@jayc88</a> for making this happen. </p>
<p>Tested on Linux and Windows and should work with any host with docker installed.</p>
<pre><code class="bash language-bash"># 1. clone the repo on in a directory on a drive with plenty of free space
git clone git@github.com:jasonppy/VoiceCraft.git
cd VoiceCraft

# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...

# 3. Try to start an existing container otherwise create a new one passing in all GPUs
./start-jupyter.sh  # linux
start-jupyter.bat   # windows

# 4. now open a webpage on the host box to the URL shown at the bottom of:
docker logs jupyter

# 5. optionally look inside from another terminal
docker exec -it jupyter /bin/bash
export USER=(your_linux_username_used_above)
export HOME=/home/$USER
sudo apt-get update

# 6. confirm video card(s) are visible inside container
nvidia-smi

# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time
echo GOOD LUCK
</code></pre>
<h2 id="environmentsetup">Environment setup</h2>
<pre><code class="bash language-bash">conda create -n voicecraft python=3.9.16
conda activate voicecraft

pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201
apt-get install ffmpeg # if you don't already have ffmpeg installed
pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft
apt-get install espeak-ng # backend for the phonemizer installed below
pip install tensorboard==2.16.2
pip install phonemizer==3.2.1
pip install torchaudio==2.0.2
pip install datasets==2.16.0
pip install torchmetrics==0.11.1
# install MFA for getting forced-alignment, this could take a few minutes
conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068
# conda install pocl # above gives an warning for installing pocl, not sure if really need this

# to run ipynb
conda install -n voicecraft ipykernel --update-deps --force-reinstall
</code></pre>
<p>If you have encountered version issues when running things, checkout <a href="./environment.yml">environment.yml</a> for exact matching.</p>
<h2 id="inferenceexamples">Inference Examples</h2>
<p>Checkout <a href="./inference_speech_editing.ipynb"><code>inference_speech_editing.ipynb</code></a> and <a href="./inference_tts.ipynb"><code>inference_tts.ipynb</code></a></p>
<h2 id="training">Training</h2>
<p>To train an VoiceCraft model, you need to prepare the following parts: </p>
<ol>
<li>utterances and their transcripts</li>
<li>encode the utterances into codes using e.g. Encodec</li>
<li>convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)</li>
<li>manifest (i.e. metadata)</li>
</ol>
<p>Step 1,2,3 are handled in <a href="./data/phonemize_encodec_encode_hf.py">./data/phonemize<em>encodec</em>encode_hf.py</a>, where </p>
<ol>
<li>Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)</li>
<li>phoneme sequence and encodec codes are also extracted using the script.</li>
</ol>
<p>An example run:</p>
<pre><code class="bash language-bash">conda activate voicecraft
export CUDA_VISIBLE_DEVICES=0
cd ./data
python phonemize_encodec_encode_hf.py \
--dataset_size xs \
--download_to path/to/store_huggingface_downloads \
--save_dir path/to/store_extracted_codes_and_phonemes \
--encodec_model_path path/to/encodec_model \
--mega_batch_size 120 \
--batch_size 32 \
--max_len 30000
</code></pre>
<p>where encodec<em>model</em>path is avaliable <a href="https://huggingface.co/pyp1/VoiceCraft">here</a>. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our <a href="https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf">paper</a>. If you encounter OOM during extraction, try decrease the batch<em>size and/or max</em>len.
The extracted codes, phonemes, and vocab.txt will be stored at <code>path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}</code>.</p>
<p>As for manifest, please download train.txt and validation.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a>, and put them under <code>path/to/store_extracted_codes_and_phonemes/manifest/</code>. Please also download vocab.txt from <a href="https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main">here</a> if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same). </p>
<p>Now, you are good to start training!</p>
<pre><code class="bash language-bash">conda activate voicecraft
cd ./z_scripts
bash e830M.sh
</code></pre>
<h2 id="license">License</h2>
<p>The codebase is under CC BY-NC-SA 4.0 (<a href="./LICENSE-CODE">LICENSE-CODE</a>), and the model weights are under Coqui Public Model License 1.0.0 (<a href="./LICENSE-MODEL">LICENSE-MODEL</a>). Note that we use some of the code from other repository that are under different licenses: <code>./models/codebooks_patterns.py</code> is under MIT license; <code>./models/modules</code>, <code>./steps/optim.py</code>, <code>data/tokenizer.py</code> are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try <a href="https://github.com/roedoejet/g2p">g2p</a> (MIT License) or <a href="https://github.com/NeuralVox/OpenPhonemizer">OpenPhonemizer</a> (BSD-3-Clause Clear), although these are not tested.</p>
<!-- How to use g2p to convert english text into IPA phoneme sequence
first install it with `pip install g2p`<pre><code class="python language-python">from g2p import make_g2p
transducer = make_g2p('eng', 'eng-ipa')
transducer("hello").output_string 
# it will output: 'h ålo ä'
</code></pre> -->
<h2 id="acknowledgement">Acknowledgement</h2>
<p>We thank Feiteng for his <a href="https://github.com/lifeiteng/vall-e">VALL-E reproduction</a>, and we thank audiocraft team for open-sourcing <a href="https://github.com/facebookresearch/audiocraft">encodec</a>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{peng2024voicecraft,
  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},
  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},
  journal   = {arXiv},
  year      = {2024},
}
</code></pre>
<h2 id="disclaimer">Disclaimer</h2>
<p>Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone's speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.</p>
<pre><code class="python language-python">print("hello world")
</code></pre>
    
</body>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
</html>